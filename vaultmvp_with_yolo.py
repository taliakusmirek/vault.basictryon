# -*- coding: utf-8 -*-
"""vaultMVP with YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-trX3x_OOxy3T2UJl_Ee0foI3MNIjp8e
"""

!pip install ultralytics kaggle torchvision roboflow pytorch3d opencv-python numpy matplotlib roboflow
!pip install "git+https://github.com/facebookresearch/pytorch3d.git"

# 1. SETUP ENVIRONMENT
from google.colab import files
import os
import shutil
import yaml

# 2. PROPER KAGGLE AUTHENTICATION (FIXED)
# First upload kaggle.json properly
#files.upload()
!mkdir -p /root/.kaggle
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# 3. DOWNLOAD DATASET (WITH ERROR HANDLING)
!kaggle datasets download -d nguyngiabol/colorful-fashion-dataset-for-object-detection --force
!unzip -q -o colorful-fashion-dataset-for-object-detection.zip -d dataset
!pip install roboflow

# 4. VERIFY FOLDER STRUCTURE
print("\nCurrent directory structure:")
!ls -R /content/dataset/colorful_fashion_dataset_for_object_detection | grep ":$" --color=never

# 5. FIXED PATH HANDLING
dataset_root = "/content/dataset/colorful_fashion_dataset_for_object_detection"
annotations_dir = os.path.join(dataset_root, "Annotations_txt")  # Using TXT annotations
images_dir = os.path.join(dataset_root, "JPEGImages")  # Note the actual folder is JPEGImages not Images

# Create Roboflow structure
os.makedirs("roboflow_dataset/train/images", exist_ok=True)
os.makedirs("roboflow_dataset/train/labels", exist_ok=True)

# 6. COPY FILES WITH PROPER PATHS
copied_pairs = 0
for txt_file in os.listdir(annotations_dir):
    if txt_file.endswith(".txt"):
        base_name = os.path.splitext(txt_file)[0]
        img_path = os.path.join(images_dir, f"{base_name}.jpg")
        ann_path = os.path.join(annotations_dir, txt_file)

        if os.path.exists(img_path) and os.path.exists(ann_path):
            shutil.copy(img_path, "roboflow_dataset/train/images/")
            shutil.copy(ann_path, "roboflow_dataset/train/labels/")
            copied_pairs += 1
        else:
            print(f"Missing pair for: {base_name}")

print(f"\nSuccessfully copied {copied_pairs} image/annotation pairs")

# 7. CREATE DATA.YAML
classes = ["sunglass", "hat", "jacket", "shirt", "pants",
           "shorts", "skirt", "dress", "bag", "shoe"]

yaml_content = f"""train: ../train/images
val: ../train/images  # Using same for demo
test: ../train/images # Using same for demo

nc: {len(classes)}
names: {classes}"""

with open("roboflow_dataset/data.yaml", "w") as f:
    f.write(yaml_content)

# 8. VERIFY FINAL STRUCTURE
print("\nFinal Roboflow dataset contents:")
!ls -lh roboflow_dataset/train/images/ | head -5
!ls -lh roboflow_dataset/train/labels/ | head -5
print(f"\nTotal images: {len(os.listdir('roboflow_dataset/train/images/'))}")
print(f"Total labels: {len(os.listdir('roboflow_dataset/train/labels/'))}")





# Verify dataset structure
print("Verifying dataset structure...")
required_folders = ["images", "labels"]
for folder in required_folders:
    path = f"roboflow_dataset/train/{folder}"
    if not os.path.exists(path):
        raise FileNotFoundError(f"Missing folder: {path}")
    print(f"Found {len(os.listdir(path))} files in {path}")

# Verify sample annotations
with open("roboflow_dataset/train/labels/100034.txt") as f:
    print("\nSample annotation:", f.read().strip())






# 1. FIRST ZIP THE DATASET PROPERLY
!cd roboflow_dataset && zip -qr ../roboflow_dataset.zip *

# 2. UPLOAD WITH PROPER PARAMETERS
from roboflow import Roboflow
import time

rf = Roboflow(api_key="9XYDbYLIX53q6YTrWWeP")
workspace = rf.workspace()

# Create project (with error handling)
try:
    project = workspace.create_project(
        project_name="fashion-ar-tryon-final",
        project_type="object-detection",
        annotation="clothing",
        project_license="MIT"
    )
    print("Created new project")
except Exception as e:
    print(f"Project likely exists: {str(e)}")
    project = workspace.project("fashion-ar-tryon-final")
    print("Using existing project")

# 3. UPLOAD VIA CLI (BYPASSES PYTHON API ISSUES)
print("\n⏳ Uploading dataset...")
project.upload(
    image_path="roboflow_dataset/train/images",
    annotation_path="roboflow_dataset/train/labels"
)

print("\n✅ Upload complete! Check your dashboard:")

!pip install ultralytics roboflow --upgrade

# 1. SETUP ENVIRONMENT (same as before)
import torch
import numpy as np
import cv2
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
    TexturesVertex, FoVPerspectiveCameras,
    MeshRenderer, MeshRasterizer,
    RasterizationSettings, SoftPhongShader, BlendParams
)
from ultralytics import YOLO
from roboflow import Roboflow
import os
import yaml
import random
from IPython.display import display, Image
import shutil

# 2. DOWNLOAD DATASET
rf = Roboflow(api_key="9XYDbYLIX53q6YTrWWeP")
project = rf.workspace().project("fashion-ar-tryon-final")
dataset = project.version(1).download("yolov8")
dataset_path = dataset.location

# 3. SIMPLIFIED YAML SETUP (use only train folder)
yaml_path = f"{dataset_path}/data.yaml"

# Update data.yaml to use only train folder for both training and validation
with open(yaml_path) as f:
    data = yaml.safe_load(f)

data['train'] = f"{dataset_path}/train/images"
data['val'] = f"{dataset_path}/train/images"  # Same as train for demo
data['test'] = f"{dataset_path}/train/images"  # Same as train for demo

with open(yaml_path, 'w') as f:
    yaml.dump(data, f)

print("Simplified data.yaml for demo:")
!cat {yaml_path}

# 4. TRAIN WITH JUST TRAINING DATA
model = YOLO("yolov8n.pt")  # Regular detection model

# Reduced epochs for demo
results = model.train(
    data=yaml_path,
    epochs=10,  # Reduced for demo
    imgsz=640,
    batch=16,
    device='cpu',
    name='fashion_demo'
)

# 5. VERIFY TRAINING
print("\nTraining complete! Results:")
!ls runs/detect/fashion_demo/weights
display(Image(filename=f'runs/detect/fashion_demo/results.png'))

!zip -r fashion-ar-tryon-final-1.zip fashion-ar-tryon-final-1/
files.download("fashion-ar-tryon-final-1")
!zip -r runs.zip runs/
files.download("runs")
files.download("yolov8n.pt")

import torch
import numpy as np
import cv2
from pytorch3d.structures import Meshes
from pytorch3d.renderer import (
    TexturesVertex, FoVPerspectiveCameras,
    MeshRenderer, MeshRasterizer,
    RasterizationSettings, SoftPhongShader, BlendParams
)
from ultralytics import YOLO

# Initialize YOLO model with trained weights
model = YOLO('runs/detect/fashion_demo/weights/best.pt')  # Load trained model

def setup_renderer(image_size=640, device='cpu'):
    """Set up PyTorch3D renderer for 640x640 resolution"""
    cameras = FoVPerspectiveCameras(
        znear=0.1, zfar=10.0, aspect_ratio=1.0, fov=60, device=device
    )

    raster_settings = RasterizationSettings(
        image_size=image_size,
        blur_radius=0.0,
        faces_per_pixel=1,
        bin_size=0
    )

    return MeshRenderer(
        rasterizer=MeshRasterizer(
            cameras=cameras,
            raster_settings=raster_settings
        ),
        shader=SoftPhongShader(
            device=device,
            cameras=cameras,
            blend_params=BlendParams(sigma=1e-4, gamma=1e-4)
        )
    )

def create_body_mesh(device='cpu'):
    """Create simplified upper body mesh"""
    verts = np.array([
        # Front vertices
        [-0.25, -0.8, 0], [0.25, -0.8, 0], [-0.35, -0.1, 0], [0.35, -0.1, 0],
        [-0.25, 0.1, 0], [0.25, 0.1, 0], [0, 0.2, 0],
        # Back vertices
        [-0.25, -0.8, 0.2], [0.25, -0.8, 0.2], [-0.35, -0.1, 0.2], [0.35, -0.1, 0.2],
        [-0.25, 0.1, 0.2], [0.25, 0.1, 0.2], [0, 0.2, 0.2]
    ], dtype=np.float32)

    faces = np.array([
        [0,1,2], [1,3,2], [2,3,4], [3,5,4], [4,5,6],
        [7,9,8], [8,9,10], [9,11,10], [10,11,12], [11,12,13],
        [0,7,1], [1,7,8], [1,8,3], [3,8,10], [3,10,5],
        [5,10,12], [5,12,6], [6,12,13], [6,13,4], [4,13,11],
        [4,11,2], [2,11,9], [2,9,0], [0,9,7]
    ], dtype=np.int64)

    skin_color = np.tile([0.9, 0.8, 0.7], (len(verts), 1))
    skin_color += np.random.uniform(-0.05, 0.05, (len(verts), 3))
    skin_color = np.clip(skin_color, 0, 1)

    return Meshes(
        verts=[torch.tensor(verts, dtype=torch.float32, device=device)],
        faces=[torch.tensor(faces, dtype=torch.int64, device=device)],
        textures=TexturesVertex(
            verts_features=torch.tensor(skin_color, dtype=torch.float32, device=device)
        )
    )

def detection_to_mesh(detection, image, depth=0.1, device='cpu'):
    """Convert YOLO detection to 3D clothing mesh"""
    boxes = detection.boxes.xywhn.cpu().numpy()
    class_ids = detection.boxes.cls.cpu().numpy().astype(int)

    meshes = []
    for box, class_id in zip(boxes, class_ids):
        xc, yc, w, h = box

        # Create 3D box vertices
        verts = np.array([
            [xc-w/2, yc-h/2, 0], [xc+w/2, yc-h/2, 0], [xc+w/2, yc+h/2, 0], [xc-w/2, yc+h/2, 0],
            [xc-w/2, yc-h/2, depth], [xc+w/2, yc-h/2, depth], [xc+w/2, yc+h/2, depth], [xc-w/2, yc+h/2, depth]
        ], dtype=np.float32)

        faces = np.array([
            [0,1,2], [0,2,3], [4,5,6], [4,6,7],
            [0,1,5], [0,5,4], [1,2,6], [1,6,5],
            [2,3,7], [2,7,6], [3,0,4], [3,4,7]
        ], dtype=np.int64)

        # Get clothing color from detected region
        h_img, w_img = image.shape[:2]
        x_min = max(0, int((xc-w/2)*w_img))
        x_max = min(w_img-1, int((xc+w/2)*w_img))
        y_min = max(0, int((yc-h/2)*h_img))
        y_max = min(h_img-1, int((yc+h/2)*h_img))

        box_region = image[y_min:y_max, x_min:x_max]
        if box_region.size > 0:
            color = np.median(box_region.reshape(-1,3), axis=0)/255.0
        else:
            color = np.array([0.5, 0.5, 0.8])  # Default blue

        meshes.append(Meshes(
            verts=[torch.tensor(verts, dtype=torch.float32, device=device)],
            faces=[torch.tensor(faces, dtype=torch.int64, device=device)],
            textures=TexturesVertex(
                verts_features=torch.tensor(
                    np.tile(color, (8, 1)), dtype=torch.float32, device=device)
            )
        ))
    return meshes

def capture_clothing():
    """Capture clothing item from webcam"""
    cap = cv2.VideoCapture(0)
    while True:
        ret, frame = cap.read()
        if not ret: break

        frame = cv2.flip(frame, 1)
        frame = cv2.resize(frame, (640, 640))

        results = model(frame)
        annotated = results[0].plot()
        cv2.imshow("Point item at camera (Press 'S' to capture)", annotated)

        if cv2.waitKey(1) == ord('s') and len(results[0].boxes) > 0:
            meshes = detection_to_mesh(results[0], frame)
            cap.release()
            cv2.destroyAllWindows()
            return meshes, results[0].names[0]

    cap.release()
    cv2.destroyAllWindows()
    return None, None

def virtual_try_on(body_mesh, clothing_meshes):
    """AR try-on experience"""
    renderer = setup_renderer()
    cap = cv2.VideoCapture(0)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break

        frame = cv2.flip(frame, 1)
        h, w = frame.shape[:2]

        # Face detection for positioning
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        if len(faces) > 0:
            x, y, fw, fh = faces[0]
            for mesh in clothing_meshes:
                verts = mesh.verts_list()[0]
                scale = fw / 200
                verts[:, :2] = (verts[:, :2] - verts[:, :2].mean(0)) * scale
                verts[:, 0] += (x + fw/2)/w - 0.5
                verts[:, 1] += (y + fh)/h - 0.3

        # Render AR overlay
        rendered = render_scene(renderer, [body_mesh] + clothing_meshes)
        rendered_rgb = rendered[0, ..., :3].cpu().numpy()
        rendered_alpha = rendered[0, ..., 3].cpu().numpy() > 0.1
        frame[rendered_alpha] = rendered_rgb[rendered_alpha] * 255

        cv2.imshow('Virtual Try-On (ESC to exit)', frame)
        if cv2.waitKey(1) == 27: break

    cap.release()
    cv2.destroyAllWindows()

def render_scene(renderer, meshes):
    """Render combined meshes"""
    combined_verts = []
    combined_faces = []
    combined_textures = []
    face_offset = 0

    for mesh in meshes:
        verts = mesh.verts_list()[0]
        faces = mesh.faces_list()[0] + face_offset
        textures = mesh.textures.verts_features_list()[0]

        combined_verts.append(verts)
        combined_faces.append(faces)
        combined_textures.append(textures)
        face_offset += verts.shape[0]

    combined_mesh = Meshes(
        verts=[torch.cat(combined_verts, dim=0)],
        faces=[torch.cat(combined_faces, dim=0)],
        textures=TexturesVertex(verts_features=[torch.cat(combined_textures, dim=0)]))

    return renderer(combined_mesh)

# Main execution
if __name__ == "__main__":
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Create body and capture clothing
    body = create_body_mesh(device)
    clothing_meshes, item_name = capture_clothing()

    if clothing_meshes:
        clothing_meshes = [m.to(device) for m in clothing_meshes]
        print(f"Trying on: {item_name}")
        virtual_try_on(body, clothing_meshes)
    else:
        print("No clothing item detected")