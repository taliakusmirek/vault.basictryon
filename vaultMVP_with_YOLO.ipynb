{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Gzk_I5LvNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03240bf6-3d52-440d-a843-c5508cb34f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.102-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.60-py3-none-any.whl.metadata (9.7 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 8.0.10 Requires-Python >=3.7,<=3.11; 8.0.11 Requires-Python >=3.7,<=3.11; 8.0.12 Requires-Python >=3.7,<=3.11; 8.0.13 Requires-Python >=3.7,<=3.11; 8.0.14 Requires-Python >=3.7,<=3.11; 8.0.15 Requires-Python >=3.7,<=3.11; 8.0.16 Requires-Python >=3.7,<=3.11; 8.0.17 Requires-Python >=3.7,<=3.11; 8.0.18 Requires-Python >=3.7,<=3.11; 8.0.19 Requires-Python >=3.7,<=3.11; 8.0.20 Requires-Python >=3.7,<=3.11; 8.0.21 Requires-Python >=3.7,<=3.11; 8.0.22 Requires-Python >=3.7,<=3.11; 8.0.23 Requires-Python >=3.7,<=3.11; 8.0.24 Requires-Python >=3.7,<=3.11; 8.0.25 Requires-Python >=3.7,<=3.11; 8.0.26 Requires-Python >=3.7,<=3.11; 8.0.27 Requires-Python >=3.7,<=3.11; 8.0.28 Requires-Python >=3.7,<=3.11; 8.0.29 Requires-Python >=3.7,<=3.11; 8.0.30 Requires-Python >=3.7,<=3.11; 8.0.31 Requires-Python >=3.7,<=3.11; 8.0.32 Requires-Python >=3.7,<=3.11; 8.0.33 Requires-Python >=3.7,<=3.11; 8.0.34 Requires-Python >=3.7,<=3.11; 8.0.7 Requires-Python >=3.7,<=3.11; 8.0.8 Requires-Python >=3.7,<=3.11; 8.0.9 Requires-Python >=3.7,<=3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch3d\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/facebookresearch/pytorch3d.git\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git to /tmp/pip-req-build-7cwgfr6v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-7cwgfr6v\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 0a59450f0ebbe12d9a8db3de937814932517633b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath (from pytorch3d==0.7.8)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.13.0)\n",
            "Collecting portalocker (from iopath->pytorch3d==0.7.8)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: pytorch3d, iopath\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl size=54807358 sha256=3421bae3217ff170b1135e02557904cad61376fabcc945c2ea70ca37981568b3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_2o97wa4/wheels/39/02/3b/eab9735f985044755f4e6d9e8473bfb8b68dc63723658e2ac2\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=3e20480a368e3a15fca25b5835844e1656c759cf2a4a316b243e56fea96dda87\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built pytorch3d iopath\n",
            "Installing collected packages: portalocker, iopath, pytorch3d\n",
            "Successfully installed iopath-0.1.10 portalocker-3.1.1 pytorch3d-0.7.8\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics kaggle torchvision roboflow pytorch3d opencv-python numpy matplotlib roboflow\n",
        "!pip install \"git+https://github.com/facebookresearch/pytorch3d.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb7sTRbpzNq0"
      },
      "outputs": [],
      "source": [
        "# 1. SETUP ENVIRONMENT\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "\n",
        "# 2. PROPER KAGGLE AUTHENTICATION (FIXED)\n",
        "# First upload kaggle.json properly\n",
        "#files.upload()\n",
        "!mkdir -p /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# 3. DOWNLOAD DATASET (WITH ERROR HANDLING)\n",
        "!kaggle datasets download -d nguyngiabol/colorful-fashion-dataset-for-object-detection --force\n",
        "!unzip -q -o colorful-fashion-dataset-for-object-detection.zip -d dataset\n",
        "!pip install roboflow\n",
        "\n",
        "# 4. VERIFY FOLDER STRUCTURE\n",
        "print(\"\\nCurrent directory structure:\")\n",
        "!ls -R /content/dataset/colorful_fashion_dataset_for_object_detection | grep \":$\" --color=never\n",
        "\n",
        "# 5. FIXED PATH HANDLING\n",
        "dataset_root = \"/content/dataset/colorful_fashion_dataset_for_object_detection\"\n",
        "annotations_dir = os.path.join(dataset_root, \"Annotations_txt\")  # Using TXT annotations\n",
        "images_dir = os.path.join(dataset_root, \"JPEGImages\")  # Note the actual folder is JPEGImages not Images\n",
        "\n",
        "# Create Roboflow structure\n",
        "os.makedirs(\"roboflow_dataset/train/images\", exist_ok=True)\n",
        "os.makedirs(\"roboflow_dataset/train/labels\", exist_ok=True)\n",
        "\n",
        "# 6. COPY FILES WITH PROPER PATHS\n",
        "copied_pairs = 0\n",
        "for txt_file in os.listdir(annotations_dir):\n",
        "    if txt_file.endswith(\".txt\"):\n",
        "        base_name = os.path.splitext(txt_file)[0]\n",
        "        img_path = os.path.join(images_dir, f\"{base_name}.jpg\")\n",
        "        ann_path = os.path.join(annotations_dir, txt_file)\n",
        "\n",
        "        if os.path.exists(img_path) and os.path.exists(ann_path):\n",
        "            shutil.copy(img_path, \"roboflow_dataset/train/images/\")\n",
        "            shutil.copy(ann_path, \"roboflow_dataset/train/labels/\")\n",
        "            copied_pairs += 1\n",
        "        else:\n",
        "            print(f\"Missing pair for: {base_name}\")\n",
        "\n",
        "print(f\"\\nSuccessfully copied {copied_pairs} image/annotation pairs\")\n",
        "\n",
        "# 7. CREATE DATA.YAML\n",
        "classes = [\"sunglass\", \"hat\", \"jacket\", \"shirt\", \"pants\",\n",
        "           \"shorts\", \"skirt\", \"dress\", \"bag\", \"shoe\"]\n",
        "\n",
        "yaml_content = f\"\"\"train: ../train/images\n",
        "val: ../train/images  # Using same for demo\n",
        "test: ../train/images # Using same for demo\n",
        "\n",
        "nc: {len(classes)}\n",
        "names: {classes}\"\"\"\n",
        "\n",
        "with open(\"roboflow_dataset/data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "# 8. VERIFY FINAL STRUCTURE\n",
        "print(\"\\nFinal Roboflow dataset contents:\")\n",
        "!ls -lh roboflow_dataset/train/images/ | head -5\n",
        "!ls -lh roboflow_dataset/train/labels/ | head -5\n",
        "print(f\"\\nTotal images: {len(os.listdir('roboflow_dataset/train/images/'))}\")\n",
        "print(f\"Total labels: {len(os.listdir('roboflow_dataset/train/labels/'))}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Verify dataset structure\n",
        "print(\"Verifying dataset structure...\")\n",
        "required_folders = [\"images\", \"labels\"]\n",
        "for folder in required_folders:\n",
        "    path = f\"roboflow_dataset/train/{folder}\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing folder: {path}\")\n",
        "    print(f\"Found {len(os.listdir(path))} files in {path}\")\n",
        "\n",
        "# Verify sample annotations\n",
        "with open(\"roboflow_dataset/train/labels/100034.txt\") as f:\n",
        "    print(\"\\nSample annotation:\", f.read().strip())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. FIRST ZIP THE DATASET PROPERLY\n",
        "!cd roboflow_dataset && zip -qr ../roboflow_dataset.zip *\n",
        "\n",
        "# 2. UPLOAD WITH PROPER PARAMETERS\n",
        "from roboflow import Roboflow\n",
        "import time\n",
        "\n",
        "rf = Roboflow(api_key=\"")\n",
        "workspace = rf.workspace()\n",
        "\n",
        "# Create project (with error handling)\n",
        "try:\n",
        "    project = workspace.create_project(\n",
        "        project_name=\"fashion-ar-tryon-final\",\n",
        "        project_type=\"object-detection\",\n",
        "        annotation=\"clothing\",\n",
        "        project_license=\"MIT\"\n",
        "    )\n",
        "    print(\"Created new project\")\n",
        "except Exception as e:\n",
        "    print(f\"Project likely exists: {str(e)}\")\n",
        "    project = workspace.project(\"fashion-ar-tryon-final\")\n",
        "    print(\"Using existing project\")\n",
        "\n",
        "# 3. UPLOAD VIA CLI (BYPASSES PYTHON API ISSUES)\n",
        "print(\"\\nâ³ Uploading dataset...\")\n",
        "project.upload(\n",
        "    image_path=\"roboflow_dataset/train/images\",\n",
        "    annotation_path=\"roboflow_dataset/train/labels\"\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Upload complete! Check your dashboard:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ64p97mehYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56419439-24d1-4c0b-f661-f38cfc8c55e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Using cached ultralytics-8.3.102-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting roboflow\n",
            "  Using cached roboflow-1.1.60-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.102-py3-none-any.whl (993 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m993.8/993.8 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading roboflow-1.1.60-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m131.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, roboflow, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed filetype-1.2.0 idna-3.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.60 ultralytics-8.3.102 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics roboflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBtJ4f9AL-u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c848e0-db49-4ed1-a419-adb7a7140613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in fashion-ar-tryon-final-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142163/142163 [00:02<00:00, 61459.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to fashion-ar-tryon-final-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5370/5370 [00:00<00:00, 6658.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified data.yaml for demo:\n",
            "names:\n",
            "- '0'\n",
            "- '1'\n",
            "- '2'\n",
            "- '3'\n",
            "- '4'\n",
            "- '5'\n",
            "- '6'\n",
            "- '7'\n",
            "- '8'\n",
            "- '9'\n",
            "nc: 10\n",
            "roboflow:\n",
            "  license: MIT\n",
            "  project: fashion-ar-tryon-final\n",
            "  url: https://universe.roboflow.com/vault-l0url/fashion-ar-tryon-final/dataset/1\n",
            "  version: 1\n",
            "  workspace: vault-l0url\n",
            "test: /content/fashion-ar-tryon-final-1/train/images\n",
            "train: /content/fashion-ar-tryon-final-1/train/images\n",
            "val: /content/fashion-ar-tryon-final-1/train/images\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 93.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.102 ğŸš€ Python-3.11.11 torch-2.6.0+cu124 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/fashion-ar-tryon-final-1/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=fashion_demo, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/fashion_demo\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 20.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/fashion_demo', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/fashion-ar-tryon-final-1/train/labels... 2682 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2682/2682 [00:01<00:00, 1724.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/fashion-ar-tryon-final-1/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/fashion-ar-tryon-final-1/train/labels.cache... 2682 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2682/2682 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/fashion_demo/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/fashion_demo\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10         0G       1.37      2.705      1.467         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [22:49<00:00,  8.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [09:05<00:00,  6.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       2682      10333      0.607      0.634       0.64      0.391\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10         0G      1.284      1.716      1.364         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [24:04<00:00,  8.60s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [09:10<00:00,  6.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       2682      10333       0.71      0.636      0.661      0.411\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10         0G      1.262      1.537      1.356         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [22:40<00:00,  8.10s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [08:50<00:00,  6.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       2682      10333      0.678      0.686      0.716      0.458\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10         0G      1.239      1.398      1.336         34        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [23:00<00:00,  8.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [08:14<00:00,  5.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       2682      10333      0.707       0.69       0.74      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10         0G      1.201      1.273       1.31         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [22:35<00:00,  8.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [08:37<00:00,  6.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       2682      10333      0.774      0.717      0.768      0.508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10         0G      1.174      1.181      1.292         59        640:  32%|â–ˆâ–ˆâ–ˆâ–      | 54/168 [07:17<15:16,  8.04s/it]"
          ]
        }
      ],
      "source": [
        "# 1. SETUP ENVIRONMENT (same as before)\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.renderer import (\n",
        "    TexturesVertex, FoVPerspectiveCameras,\n",
        "    MeshRenderer, MeshRasterizer,\n",
        "    RasterizationSettings, SoftPhongShader, BlendParams\n",
        ")\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "import os\n",
        "import yaml\n",
        "import random\n",
        "from IPython.display import display, Image\n",
        "import shutil\n",
        "\n",
        "# 2. DOWNLOAD DATASET\n",
        "rf = Roboflow(api_key=\"9XYDbYLIX53q6YTrWWeP\")\n",
        "project = rf.workspace().project(\"fashion-ar-tryon-final\")\n",
        "dataset = project.version(1).download(\"yolov8\")\n",
        "dataset_path = dataset.location\n",
        "\n",
        "# 3. SIMPLIFIED YAML SETUP (use only train folder)\n",
        "yaml_path = f\"{dataset_path}/data.yaml\"\n",
        "\n",
        "# Update data.yaml to use only train folder for both training and validation\n",
        "with open(yaml_path) as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "data['train'] = f\"{dataset_path}/train/images\"\n",
        "data['val'] = f\"{dataset_path}/train/images\"  # Same as train for demo\n",
        "data['test'] = f\"{dataset_path}/train/images\"  # Same as train for demo\n",
        "\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(data, f)\n",
        "\n",
        "print(\"Simplified data.yaml for demo:\")\n",
        "!cat {yaml_path}\n",
        "\n",
        "# 4. TRAIN WITH JUST TRAINING DATA\n",
        "model = YOLO(\"yolov8n.pt\")  # Regular detection model\n",
        "\n",
        "# Reduced epochs for demo\n",
        "results = model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=10,  # Reduced for demo\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    device='cpu',\n",
        "    name='fashion_demo'\n",
        ")\n",
        "\n",
        "# 5. VERIFY TRAINING\n",
        "print(\"\\nTraining complete! Results:\")\n",
        "!ls runs/detect/fashion_demo/weights\n",
        "display(Image(filename=f'runs/detect/fashion_demo/results.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kyb-Z_0n3MQk"
      },
      "outputs": [],
      "source": [
        "!zip -r fashion-ar-tryon-final-1.zip fashion-ar-tryon-final-1/\n",
        "files.download(\"fashion-ar-tryon-final-1\")\n",
        "!zip -r runs.zip runs/\n",
        "files.download(\"runs\")\n",
        "files.download(\"yolov8n.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLD-ri9B1h8g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.renderer import (\n",
        "    TexturesVertex, FoVPerspectiveCameras,\n",
        "    MeshRenderer, MeshRasterizer,\n",
        "    RasterizationSettings, SoftPhongShader, BlendParams\n",
        ")\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Initialize YOLO model with trained weights\n",
        "model = YOLO('runs/detect/fashion_demo/weights/best.pt')  # Load trained model\n",
        "\n",
        "def setup_renderer(image_size=640, device='cpu'):\n",
        "    \"\"\"Set up PyTorch3D renderer for 640x640 resolution\"\"\"\n",
        "    cameras = FoVPerspectiveCameras(\n",
        "        znear=0.1, zfar=10.0, aspect_ratio=1.0, fov=60, device=device\n",
        "    )\n",
        "\n",
        "    raster_settings = RasterizationSettings(\n",
        "        image_size=image_size,\n",
        "        blur_radius=0.0,\n",
        "        faces_per_pixel=1,\n",
        "        bin_size=0\n",
        "    )\n",
        "\n",
        "    return MeshRenderer(\n",
        "        rasterizer=MeshRasterizer(\n",
        "            cameras=cameras,\n",
        "            raster_settings=raster_settings\n",
        "        ),\n",
        "        shader=SoftPhongShader(\n",
        "            device=device,\n",
        "            cameras=cameras,\n",
        "            blend_params=BlendParams(sigma=1e-4, gamma=1e-4)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def create_body_mesh(device='cpu'):\n",
        "    \"\"\"Create simplified upper body mesh\"\"\"\n",
        "    verts = np.array([\n",
        "        # Front vertices\n",
        "        [-0.25, -0.8, 0], [0.25, -0.8, 0], [-0.35, -0.1, 0], [0.35, -0.1, 0],\n",
        "        [-0.25, 0.1, 0], [0.25, 0.1, 0], [0, 0.2, 0],\n",
        "        # Back vertices\n",
        "        [-0.25, -0.8, 0.2], [0.25, -0.8, 0.2], [-0.35, -0.1, 0.2], [0.35, -0.1, 0.2],\n",
        "        [-0.25, 0.1, 0.2], [0.25, 0.1, 0.2], [0, 0.2, 0.2]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    faces = np.array([\n",
        "        [0,1,2], [1,3,2], [2,3,4], [3,5,4], [4,5,6],\n",
        "        [7,9,8], [8,9,10], [9,11,10], [10,11,12], [11,12,13],\n",
        "        [0,7,1], [1,7,8], [1,8,3], [3,8,10], [3,10,5],\n",
        "        [5,10,12], [5,12,6], [6,12,13], [6,13,4], [4,13,11],\n",
        "        [4,11,2], [2,11,9], [2,9,0], [0,9,7]\n",
        "    ], dtype=np.int64)\n",
        "\n",
        "    skin_color = np.tile([0.9, 0.8, 0.7], (len(verts), 1))\n",
        "    skin_color += np.random.uniform(-0.05, 0.05, (len(verts), 3))\n",
        "    skin_color = np.clip(skin_color, 0, 1)\n",
        "\n",
        "    return Meshes(\n",
        "        verts=[torch.tensor(verts, dtype=torch.float32, device=device)],\n",
        "        faces=[torch.tensor(faces, dtype=torch.int64, device=device)],\n",
        "        textures=TexturesVertex(\n",
        "            verts_features=torch.tensor(skin_color, dtype=torch.float32, device=device)\n",
        "        )\n",
        "    )\n",
        "\n",
        "def detection_to_mesh(detection, image, depth=0.1, device='cpu'):\n",
        "    \"\"\"Convert YOLO detection to 3D clothing mesh\"\"\"\n",
        "    boxes = detection.boxes.xywhn.cpu().numpy()\n",
        "    class_ids = detection.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "    meshes = []\n",
        "    for box, class_id in zip(boxes, class_ids):\n",
        "        xc, yc, w, h = box\n",
        "\n",
        "        # Create 3D box vertices\n",
        "        verts = np.array([\n",
        "            [xc-w/2, yc-h/2, 0], [xc+w/2, yc-h/2, 0], [xc+w/2, yc+h/2, 0], [xc-w/2, yc+h/2, 0],\n",
        "            [xc-w/2, yc-h/2, depth], [xc+w/2, yc-h/2, depth], [xc+w/2, yc+h/2, depth], [xc-w/2, yc+h/2, depth]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        faces = np.array([\n",
        "            [0,1,2], [0,2,3], [4,5,6], [4,6,7],\n",
        "            [0,1,5], [0,5,4], [1,2,6], [1,6,5],\n",
        "            [2,3,7], [2,7,6], [3,0,4], [3,4,7]\n",
        "        ], dtype=np.int64)\n",
        "\n",
        "        # Get clothing color from detected region\n",
        "        h_img, w_img = image.shape[:2]\n",
        "        x_min = max(0, int((xc-w/2)*w_img))\n",
        "        x_max = min(w_img-1, int((xc+w/2)*w_img))\n",
        "        y_min = max(0, int((yc-h/2)*h_img))\n",
        "        y_max = min(h_img-1, int((yc+h/2)*h_img))\n",
        "\n",
        "        box_region = image[y_min:y_max, x_min:x_max]\n",
        "        if box_region.size > 0:\n",
        "            color = np.median(box_region.reshape(-1,3), axis=0)/255.0\n",
        "        else:\n",
        "            color = np.array([0.5, 0.5, 0.8])  # Default blue\n",
        "\n",
        "        meshes.append(Meshes(\n",
        "            verts=[torch.tensor(verts, dtype=torch.float32, device=device)],\n",
        "            faces=[torch.tensor(faces, dtype=torch.int64, device=device)],\n",
        "            textures=TexturesVertex(\n",
        "                verts_features=torch.tensor(\n",
        "                    np.tile(color, (8, 1)), dtype=torch.float32, device=device)\n",
        "            )\n",
        "        ))\n",
        "    return meshes\n",
        "\n",
        "def capture_clothing():\n",
        "    \"\"\"Capture clothing item from webcam\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        frame = cv2.resize(frame, (640, 640))\n",
        "\n",
        "        results = model(frame)\n",
        "        annotated = results[0].plot()\n",
        "        cv2.imshow(\"Point item at camera (Press 'S' to capture)\", annotated)\n",
        "\n",
        "        if cv2.waitKey(1) == ord('s') and len(results[0].boxes) > 0:\n",
        "            meshes = detection_to_mesh(results[0], frame)\n",
        "            cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            return meshes, results[0].names[0]\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return None, None\n",
        "\n",
        "def virtual_try_on(body_mesh, clothing_meshes):\n",
        "    \"\"\"AR try-on experience\"\"\"\n",
        "    renderer = setup_renderer()\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        # Face detection for positioning\n",
        "        face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            x, y, fw, fh = faces[0]\n",
        "            for mesh in clothing_meshes:\n",
        "                verts = mesh.verts_list()[0]\n",
        "                scale = fw / 200\n",
        "                verts[:, :2] = (verts[:, :2] - verts[:, :2].mean(0)) * scale\n",
        "                verts[:, 0] += (x + fw/2)/w - 0.5\n",
        "                verts[:, 1] += (y + fh)/h - 0.3\n",
        "\n",
        "        # Render AR overlay\n",
        "        rendered = render_scene(renderer, [body_mesh] + clothing_meshes)\n",
        "        rendered_rgb = rendered[0, ..., :3].cpu().numpy()\n",
        "        rendered_alpha = rendered[0, ..., 3].cpu().numpy() > 0.1\n",
        "        frame[rendered_alpha] = rendered_rgb[rendered_alpha] * 255\n",
        "\n",
        "        cv2.imshow('Virtual Try-On (ESC to exit)', frame)\n",
        "        if cv2.waitKey(1) == 27: break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def render_scene(renderer, meshes):\n",
        "    \"\"\"Render combined meshes\"\"\"\n",
        "    combined_verts = []\n",
        "    combined_faces = []\n",
        "    combined_textures = []\n",
        "    face_offset = 0\n",
        "\n",
        "    for mesh in meshes:\n",
        "        verts = mesh.verts_list()[0]\n",
        "        faces = mesh.faces_list()[0] + face_offset\n",
        "        textures = mesh.textures.verts_features_list()[0]\n",
        "\n",
        "        combined_verts.append(verts)\n",
        "        combined_faces.append(faces)\n",
        "        combined_textures.append(textures)\n",
        "        face_offset += verts.shape[0]\n",
        "\n",
        "    combined_mesh = Meshes(\n",
        "        verts=[torch.cat(combined_verts, dim=0)],\n",
        "        faces=[torch.cat(combined_faces, dim=0)],\n",
        "        textures=TexturesVertex(verts_features=[torch.cat(combined_textures, dim=0)]))\n",
        "\n",
        "    return renderer(combined_mesh)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create body and capture clothing\n",
        "    body = create_body_mesh(device)\n",
        "    clothing_meshes, item_name = capture_clothing()\n",
        "\n",
        "    if clothing_meshes:\n",
        "        clothing_meshes = [m.to(device) for m in clothing_meshes]\n",
        "        print(f\"Trying on: {item_name}\")\n",
        "        virtual_try_on(body, clothing_meshes)\n",
        "    else:\n",
        "        print(\"No clothing item detected\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
